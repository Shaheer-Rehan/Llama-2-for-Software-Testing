{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Finetuning Llama-2-7b for Software Test Case Generation**\n",
        "This notebook contains the complete code for finetuning of the Llama-2-7b-chat-hf model. The A100 GPU from Google Colab was used for the entirety of this project, and the settings specified in the notebook will not work unless the A100 GPU is used due to memory overheads."
      ],
      "metadata": {
        "id": "32ahGhFUwoZu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHzVlnHyawCE"
      },
      "source": [
        "## **Import Necessary Libraries and Methods**\n",
        "It is important to install the required packages before executing the code on Google Colab. The versions and order of installation of these packages needs to be maintained so no issues arise due to non-compliance of module versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "P_PA25zjYurp"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers==4.30.0 trl bitsandbytes==0.43.3 accelerate==0.21.0 peft==0.4.0 tensorboard==2.15.0 wandb\n",
        "!pip install -q cudf-cu12==24.4.1\n",
        "!pip install -q ibis-framework --upgrade\n",
        "!pip install -q bigframes --upgrade\n",
        "!pip install -q gcsfs==2024.3.1\n",
        "!pip install -q datasets==2.19.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6b4LWr6_ZYRH"
      },
      "outputs": [],
      "source": [
        "# Update the package list\n",
        "!apt-get update\n",
        "\n",
        "# Install development libraries for pycairo and other required packages\n",
        "!apt-get install -y libcairo2-dev pkg-config python3-dev\n",
        "\n",
        "# Install pycairo\n",
        "!pip install -q pycairo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "O0S5Omb9aS6C"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorboard==2.17.0\n",
        "!pip install -q requests==2.32.3\n",
        "!pip install -q xformers triton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Bv4aEqmzZowI"
      },
      "outputs": [],
      "source": [
        "!pip check\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "J2C_5rRXZpWy"
      },
      "outputs": [],
      "source": [
        "!python -m bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "W5mIFEpdaF5a"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import gc\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, BitsAndBytesConfig, LlamaForCausalLM, TrainingArguments, pipeline\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, PeftModel\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from trl import SFTTrainer\n",
        "from datasets import Dataset, concatenate_datasets, DatasetDict, load_from_disk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z7eNODGa3yl"
      },
      "source": [
        "## **Converting Text Files into Pandas Dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "77dOW6FKa7uI"
      },
      "outputs": [],
      "source": [
        "\"\"\"Firstm we assign text files for input and output to corresponding variables\"\"\"\n",
        "input_path = '/content/drive/MyDrive/MSc Project/new_attempt/data/train/input.methods.txt'\n",
        "output_path = '/content/drive/MyDrive/MSc Project/new_attempt/data/train/output.tests.txt'\n",
        "with open (input_path, 'r') as f:\n",
        "  input = f.read() # Read the input file and assign it to the input variable\n",
        "with open (output_path, 'r') as f:\n",
        "  output = f.read() # Read the output file and assign it to the output variable\n",
        "\n",
        "\"\"\"Now, we can create a pandas dataframe from the input and output text files\"\"\"\n",
        "df = pd.DataFrame({'input': input.split('\\n'), 'output': output.split('\\n')})\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TPMxGzyVi6s"
      },
      "source": [
        "## **Defining a Chat Template for the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PD2fpDXPVnk5"
      },
      "outputs": [],
      "source": [
        "\"\"\"This function is used to define a chat template for the inference phase.\n",
        "The dataframe will be mapped according to this template so that the model expects\n",
        "this format for inference\"\"\"\n",
        "def chat_template(sample):\n",
        "  return f\"---Focal Method---\\n{sample}\\n\\n---Unit Test---\\n\"\n",
        "\n",
        "df['input'] = df['input'].map(chat_template) # The input records are mapped as per the chat template\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Reducing the size of the Dataset to a Manageable Size**"
      ],
      "metadata": {
        "id": "i_esD6sSqPi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Taking a chunk (25000 entries) of the dataset for the finetuning process\n",
        "due to memory limitations. The full dataset has over 600000 records, and the finetuning process\n",
        "cannot be carried out on a single consumer grade GPU for such a large dataset.\n",
        "So, this project will serve as the baseline for future work in this domain and work as\n",
        "a proof of concept for the finetuning process\"\"\"\n",
        "df_chunk = df.iloc[:25000]\n",
        "df_chunk"
      ],
      "metadata": {
        "id": "2qjquAvWqVxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading the Model and Tokenizer**"
      ],
      "metadata": {
        "id": "HSXKEYg12n3-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkZoBQU4cMYm"
      },
      "outputs": [],
      "source": [
        "base_model = \"NousResearch/Llama-2-7b-chat-hf\" # Model from NousResearch available on Hugging Face\n",
        "new_model = \"/content/drive/MyDrive/MSc Project/new_attempt/finetuned_llama_for_software_testing\" # Specifying the directory for the new model after finetuning\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast = True) # Loading the tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70ExTA3_cLO6"
      },
      "source": [
        "## **Tokenizing the Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdA8QNe5cXYm"
      },
      "outputs": [],
      "source": [
        "\"\"\"The dataset cannot be used directly by the Llama-2 model for training, it first needs to be\n",
        "converted into tokens. This piece of code converts the dataset into tokens, and also returns list\n",
        "objects for the token lengths of the input, output, and combined (input + output) tokens.\n",
        "Batchwise processing is used during the tokenization process because this process can be\n",
        "memory intensive, and can cause the runtime to crash if the entire dataset is tokenized at once.\"\"\"\n",
        "batch_size = 5000\n",
        "final_dataset = None\n",
        "input_token_length_list = [] # Initializing lists for storing token lengths\n",
        "output_token_length_list = []\n",
        "combined_token_length_list = []\n",
        "\n",
        "for start_idx in range(0, len(df_chunk), batch_size):\n",
        "  end_idx = min(start_idx + batch_size, len(df_chunk))\n",
        "  df_chunk_batch = df_chunk.iloc[start_idx:end_idx]\n",
        "\n",
        "  # Tokenizing the batch\n",
        "  input_chunk_tokens = tokenizer(df_chunk_batch['input'].tolist(), return_tensors = 'np')\n",
        "  output_chunk_tokens = tokenizer(df_chunk_batch['output'].tolist(), return_tensors = 'np')\n",
        "\n",
        "  # Creating a list of token lengths for each row for inputs, outputs, and combined lengths\n",
        "  input_token_length = [len(token) for token in input_chunk_tokens['input_ids']]\n",
        "  output_token_length = [len(token) for token in output_chunk_tokens['input_ids']]\n",
        "  combined_token_length = [x + y for x, y in zip(input_token_length, output_token_length)]\n",
        "\n",
        "  # Appending the token length lists to the corresponding main lists outside the loop\n",
        "  input_token_length_list.extend(input_token_length)\n",
        "  output_token_length_list.extend(output_token_length)\n",
        "  combined_token_length_list.extend(combined_token_length)\n",
        "\n",
        "  # Creating separate numpy arrays\n",
        "  input_ids_np = input_chunk_tokens['input_ids']\n",
        "  attention_mask_np = input_chunk_tokens['attention_mask']\n",
        "  labels_np = output_chunk_tokens['input_ids']\n",
        "\n",
        "  # Adding the text field in the dataset for SFTTrainer\n",
        "  combined_text_list = [\n",
        "                        f\"Input: {input_text}\\nOutput: {output_text}\"\n",
        "                        for input_text, output_text in zip(df_chunk_batch['input'].tolist(), df_chunk_batch['output'].tolist())\n",
        "                       ]\n",
        "\n",
        "  # Creating a dictionary for the batch where keys are column names and values are the lists\n",
        "  # This is the format in which the model expects to receive the data during finetuning\n",
        "  tokenized_dataset_dict = {\n",
        "                            'input_ids': input_ids_np,\n",
        "                            'attention_mask': attention_mask_np,\n",
        "                            'labels': labels_np,\n",
        "                            'text': combined_text_list\n",
        "                           }\n",
        "\n",
        "  # Creating a dataset object from the batch dictionary\n",
        "  batch_dataset = Dataset.from_dict(tokenized_dataset_dict)\n",
        "\n",
        "  # Concatenating the batch datasets with the final dataset\n",
        "  if final_dataset is None:\n",
        "    final_dataset = batch_dataset\n",
        "  else:\n",
        "    final_dataset = concatenate_datasets([final_dataset, batch_dataset])\n",
        "\n",
        "  # Removing intermediate objects from the memory to make the process efficient\n",
        "  del (input_chunk_tokens, output_chunk_tokens, input_token_length, output_token_length,\n",
        "       combined_token_length, input_ids_np, attention_mask_np, labels_np,\n",
        "       combined_text_list, tokenized_dataset_dict, batch_dataset)\n",
        "  gc.collect()\n",
        "\n",
        "final_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Plotting Distribution of Token Lengths for Input, Output, and Combined Tokens**"
      ],
      "metadata": {
        "id": "QGCAobjP7dcW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qq5t-tOkvhCp"
      },
      "outputs": [],
      "source": [
        "\"\"\" Defining a function for plotting the token length distributions\"\"\"\n",
        "def plot_distribution(token_counts, title):\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.hist(token_counts, bins=50, color='#348ddb', edgecolor='black')\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xlabel(\"Number of tokens\", fontsize=14)\n",
        "    plt.ylabel(\"Number of examples\", fontsize=14)\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plotting the token length distributions for input, output, and combined token lengths\n",
        "plot_distribution(input_token_length_list, \"Distribution of Input Token Lengths\")\n",
        "plot_distribution(output_token_length_list, \"Distribution of Output Token Lengths\")\n",
        "plot_distribution(combined_token_length_list, \"Distribution of Combined Token Lengths\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ko56a7G5IVi"
      },
      "source": [
        "## **Filtering out entries with more than 1024 tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAG_UFcp5EDS"
      },
      "outputs": [],
      "source": [
        "\"\"\"The distribution plots indicate that most of the records have combined token lengths of <= 1000. So, the records\n",
        "with combined token lengths of >=1024 and input token lengths of >512 are filetered out. This is done to fix the\n",
        "long tail distribution. The context window of Llama 2 is 4096, but we will limit the token lengths to 1024 for\n",
        "computational efficiency\"\"\"\n",
        "valid_indices = [i for i, count in enumerate(combined_token_length_list) if count < 1024 and input_token_length_list[i] <= 512] # removing indices with large token lengths\n",
        "print(f\"Number of valid records: {len(valid_indices)}\")\n",
        "print(f\"So, removing {len(final_dataset) - len(valid_indices)} records\") # displaying the filtration results\n",
        "\n",
        "final_dataset = final_dataset.select(valid_indices) # updating the final dataset to include only the valid indices\n",
        "\n",
        "valid_token_lengths = [combined_token_length_list[i] for i in valid_indices] # Get combined token counts for each row in the updated dataset\n",
        "\n",
        "plot_distribution(valid_token_lengths, \"Distribution of Valid Token Lengths\") # Plotting the updated distribution of valid token lengths"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train Test Split**"
      ],
      "metadata": {
        "id": "KQPtJQZiEnZy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1d-0pYWjcjb"
      },
      "outputs": [],
      "source": [
        "\"\"\"Splitting the data for training and testing. Both datasets will be used during the finetuning process\n",
        "to determine the training loss and evaluation loss respectively.\"\"\"\n",
        "train_test_split = final_dataset.train_test_split(test_size = 0.2, seed = 42)\n",
        "train_data = train_test_split['train']\n",
        "eval_data = train_test_split['test']\n",
        "\n",
        "print(train_data)\n",
        "print(eval_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Save and Load the Datasets**"
      ],
      "metadata": {
        "id": "RC0wsDVVGCyS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZTTbrdM2r4w"
      },
      "outputs": [],
      "source": [
        "\"\"\"Finetuning requires multiple runs so the best hyperparameter configuration can be reached.\n",
        "To save time and computational resources, we save the dataset here so it can be loaded directly\n",
        "for subsequent runs\"\"\"\n",
        "dataset_dictionary = DatasetDict({'train': train_data, 'test': eval_data}) # Creating a dictionary with the dataset\n",
        "dataset_dictionary.save_to_disk('/content/drive/MyDrive/MSc Project/new_attempt/data/train/final_tokenized_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh2qRxlM3kdb"
      },
      "outputs": [],
      "source": [
        "\"\"\"Loading the dataset from the local drive. As the full dataset was saved, the train test split must be done again\"\"\"\n",
        "loaded_dataset = load_from_disk('/content/drive/MyDrive/MSc Project/new_attempt/data/train/final_tokenized_dataset')\n",
        "train_data = loaded_dataset['train']\n",
        "eval_data = loaded_dataset['test']\n",
        "\n",
        "print(train_data)\n",
        "print(eval_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgK2u25Y1pKD"
      },
      "source": [
        "## **Pad the Tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-D4xWoYg1uxI"
      },
      "outputs": [],
      "source": [
        "\"\"\"Padding the tokens is a crucial step. Padding makes it so all the records have the same number of tokens, so, the\n",
        "padding tokens are added to records with lower token counts. The UNK token is used for padding to avoid the known\n",
        "issues with the EOS token\"\"\"\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.padding_side = 'right'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPOql1PogONh"
      },
      "source": [
        "## **QLoRA Quantization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7A8CsgX0gOsp"
      },
      "outputs": [],
      "source": [
        "\"\"\"Here, we configure the settings for QLoRA quantization.\"\"\"\n",
        "qlora_config = BitsAndBytesConfig(\n",
        "                                  load_in_4bit = True,\n",
        "                                  bnb_4bit_compute_dtype = torch.float16,\n",
        "                                  bnb_4bit_quant_type = \"nf4\",  # Normalized Float 4-bit gives improved accuracy compared to standard 4-bit integer quantization\n",
        "                                  bnb_4bit_use_double_quant = False  # Setting as False to avoid further losses in accuracy at the cost of slight increase in memory usage\n",
        "                                  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWekgFTrgbpP"
      },
      "source": [
        "## **Loading the Llama 2 Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMERhaQUgcFX"
      },
      "outputs": [],
      "source": [
        "\"\"\"The Llama 2 model is loaded and prepared for 4 bit training using the QLoRA config defined earlier\"\"\"\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "                                          base_model,\n",
        "                                          quantization_config = qlora_config,\n",
        "                                          device_map = \"auto\"\n",
        "                                        )\n",
        "model.config.use_cache = False  # Setting as False to save memory during training\n",
        "model.config.pretraining_tp = 1  # Tensor parallelism set unavailable as only 1 GPU available via colab\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voyJey2EhWFo"
      },
      "source": [
        "## **Parameter-Efficient Fine-Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN16qTazhWm0"
      },
      "outputs": [],
      "source": [
        "\"\"\"\"PEFT is another strategy for model compression. The target modules are the attention layers and feedforward network\n",
        "projection layers. Targeting these means that these layers will be adapted to the new data during finetuning\"\"\"\n",
        "peft_params = LoraConfig(\n",
        "                          lora_alpha = 32,\n",
        "                          lora_dropout = 0.05,\n",
        "                          r = 16, # this is the rank of matrices to be used in the LoRA process\n",
        "                          bias = \"none\",\n",
        "                          task_type = \"CAUSAL_LM\",\n",
        "                          target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
        "                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM3xF9lkhgsJ"
      },
      "source": [
        "## **Setting the Training Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caQjuO81hhLv"
      },
      "outputs": [],
      "source": [
        "\"\"\"Specifying the training parameters\"\"\"\n",
        "training_params = TrainingArguments(\n",
        "                                      output_dir = \"/content/drive/MyDrive/MSc Project/new_attempt/results\",\n",
        "                                      num_train_epochs = 12,\n",
        "                                      per_device_train_batch_size = 32,\n",
        "                                      per_device_eval_batch_size = 32,\n",
        "                                      gradient_accumulation_steps = 1,\n",
        "                                      optim = \"paged_adamw_8bit\", # using the 8-bit optimizer\n",
        "                                      evaluation_strategy = \"steps\",\n",
        "                                      eval_steps = 500,\n",
        "                                      save_steps = 500,\n",
        "                                      logging_steps = 100,\n",
        "                                      learning_rate = 2e-4, # this is the learning rate that produced the most promising results\n",
        "                                      weight_decay = 0.001,\n",
        "                                      fp16 = True,\n",
        "                                      bf16 = False, # These settings for fp16 and bf16 ensure mixed precision is enabled for computational efficiency\n",
        "                                      max_grad_norm = 0.3,\n",
        "                                      max_steps = -1,\n",
        "                                      warmup_ratio = 0.03,\n",
        "                                      lr_scheduler_type = \"linear\", # linear decay produced better results than cosine annealing\n",
        "                                      report_to = \"wandb\"\n",
        "                                    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjH9OueFhrIQ"
      },
      "source": [
        "## **Finetuning the LLM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBIFDKpBhr4s"
      },
      "outputs": [],
      "source": [
        "\"\"\"Finetuning the base model and saving the adaption layers after finetuning\"\"\"\n",
        "trainer = SFTTrainer(\n",
        "                      model = model,\n",
        "                      train_dataset = train_data,\n",
        "                      eval_dataset = eval_data,\n",
        "                      peft_config = peft_params,\n",
        "                      max_seq_length = 512,\n",
        "                      tokenizer = tokenizer,\n",
        "                      packing = False,  # Leaving packing as False because the Java focal methods and unit tests can be lengthy\n",
        "                      dataset_text_field = 'text',\n",
        "                      args = training_params\n",
        "                    )\n",
        "\n",
        "trainer.train() # Training the model\n",
        "trainer.model.save_pretrained(new_model) # Saving the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Inference**"
      ],
      "metadata": {
        "id": "TcP8iOtTvAk0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YxTR9uh2Pui"
      },
      "outputs": [],
      "source": [
        "\"\"\"Running the text generation pipeline with the finetuned model. This is just to check if the model is working for inference\n",
        "properly. Detailed inference will be carried out for validation in the next notebook. The focal method used here was randomly\n",
        "selected for the eval dataset in the parent corpus, and is unseen data for the model\"\"\"\n",
        "prompt = \"VerificationUtil { static public boolean isZero(Number value, double zeroThreshold){ return (value.doubleValue() >= -zeroThreshold) && (value.doubleValue() <= zeroThreshold); } }\"\n",
        "focal_method = f\"---Focal Method---\\n{prompt}\\n\\n---Unit Test---\\n\"\n",
        "\n",
        "pipe = pipeline(task = \"text-generation\", model = model, tokenizer = tokenizer, max_length = 512)\n",
        "result = pipe(focal_method)\n",
        "print(result[0]['generated_text'][len(focal_method):])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9J0dkz7A4z8"
      },
      "outputs": [],
      "source": [
        "\"\"\"Emptying the VRAM so the next step can be carried out without memory issues\"\"\"\n",
        "del model\n",
        "del pipe\n",
        "del trainer\n",
        "import gc\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKxwfUeFBEx6"
      },
      "source": [
        "## **Merging the base model with the trained adapter**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DucCLp8CA8bK"
      },
      "outputs": [],
      "source": [
        "\"\"\"Reload the model in FP16 precision and merge it with the adapted LoRA weights\"\"\"\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    low_cpu_mem_usage = True,\n",
        "    return_dict = True,\n",
        "    torch_dtype = torch.float16,\n",
        "    device_map = \"auto\",\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "\"\"\"Reload the tokenizer to save it\"\"\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}