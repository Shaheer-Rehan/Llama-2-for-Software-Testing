{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Validation**\n",
        "This notebook uses the finetuned Llama-2-7b-chat-hf model and performs validation by running inference on the finetuned model using unseen data.\n",
        "The generated responses are then compared with the actual test cases and\n",
        "metrics for accuracy, precision, recall, and F1 score are calculated. This notebook can run on the L4 GPU (or better) from Google Colab"
      ],
      "metadata": {
        "id": "pat57jWl7CRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import Necessary Libraries and Methods**\n",
        "As in the previous notebook, the following packages need to be installed before\n",
        "the required libraries and modules can be imported"
      ],
      "metadata": {
        "id": "92C5XIakE6rH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers==4.30.0 accelerate==0.21.0 peft==0.4.0 jedi xformers triton tqdm\n",
        "!pip install -q cudf-cu12==24.4.1\n",
        "!pip install -q ibis-framework --upgrade\n",
        "!pip install -q bigframes --upgrade\n",
        "!pip install -q gcsfs==2024.3.1\n",
        "!pip install -q datasets==2.19.1"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ppDLM55tFBoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the package list\n",
        "!apt-get update\n",
        "\n",
        "# Install development libraries for pycairo and other required packages\n",
        "!apt-get install -y libcairo2-dev pkg-config python3-dev\n",
        "\n",
        "# Install pycairo\n",
        "!pip install -q pycairo"
      ],
      "metadata": {
        "collapsed": true,
        "id": "z80VIRSfGy0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip check ##Check for Any Issues in the Libraries"
      ],
      "metadata": {
        "id": "6Yd3Vd5CFHzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x8f7sJ-AmkP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "from transformers import AutoTokenizer, LlamaForCausalLM, pipeline\n",
        "from peft import PeftModel\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading the Base and Finetuned Model & Combining the LoRA Weights with the Base Model**\n",
        "The base model and finetuned adaption layers need to be loaded from the directories as this is a new notebook running on a fresh runtime"
      ],
      "metadata": {
        "id": "NSTKOSopI5tm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = \"NousResearch/Llama-2-7b-chat-hf\" # Loading the base model from Hugging Face\n",
        "new_model = \"/content/drive/MyDrive/MSc Project/new_attempt/finetuned_llama_for_software_testing\" # Loading the new model saved in the directory after finetuning\n",
        "\n",
        "\"\"\"Reload the base model in FP16 and merge it with LoRA weights\"\"\"\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    low_cpu_mem_usage = True,\n",
        "    return_dict = True,\n",
        "    torch_dtype = torch.float16,\n",
        "    device_map = \"auto\",\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "\"\"\"Loading the saved tokenizer\"\"\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code = True)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "GOGw8IfuI9rJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create Dataset from Validation Files**\n",
        "For validation, the test folder was used from the parent corpus. This data was not introduced to the model in any form prior to this step, so it can serve as the validation dataset"
      ],
      "metadata": {
        "id": "-q_UJpeteuBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Load the input and output text file paths\"\"\"\n",
        "input_file_path = \"/content/drive/MyDrive/MSc Project/new_attempt/data/val_data/input.methods.txt\"\n",
        "output_file_path = \"/content/drive/MyDrive/MSc Project/new_attempt/data/val_data/output.tests.txt\"\n",
        "\n",
        "with open (input_file_path, \"r\") as file:\n",
        "  input_text = file.read()\n",
        "\n",
        "with open (output_file_path, \"r\") as file:\n",
        "  output_text = file.read()\n",
        "\n",
        "\"\"\"Creating a pandas dataframe using the files\"\"\"\n",
        "df_validation = pd.DataFrame({\"input\": input_text.split(\"\\n\"), \"output\": output_text.split(\"\\n\")})\n",
        "df_validation"
      ],
      "metadata": {
        "id": "0DuWnVZPetnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Filter Out Records based on Token Length**\n",
        "The records need to be filtered based on token length for this dataset as well. To include more examples in the validation process, the max combined length was raised to 2048 and the max input length was raised to 1024. As the inference step is not as computationally expensive as finetuning, and the dataset is much smaller, the token lengths can be raised to these numbers without any issues"
      ],
      "metadata": {
        "id": "V9iKIlQm7saZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Creating a function to filter out the records with token lengths that do not\n",
        "meet the requirements. Batch wise tokenization is first performed on the dataset so the\n",
        "filtration can be done based on token lengths\"\"\"\n",
        "def filter_records(df, tokenizer, max_input_length = 1024, max_total_length = 2048, batch_size = 2000):\n",
        "  filtered_indices = []\n",
        "\n",
        "  for start_idx in range(0, len(df), batch_size):\n",
        "    end_idx = min(start_idx + batch_size, len(df))\n",
        "    df_batch = df.iloc[start_idx:end_idx]\n",
        "\n",
        "    input_batch_tokenized = tokenizer(df_batch[\"input\"].tolist(), return_tensors = \"np\", padding = True) # converting input batch into tokens\n",
        "    output_batch_tokenized = tokenizer(df_batch[\"output\"].tolist(), return_tensors = \"np\", padding = True) # converting output batch into tokens\n",
        "\n",
        "    # Create a list of token lengths for each row for inputs, outputs, and combined lengths\n",
        "    input_token_lengths = np.array([len(token) for token in input_batch_tokenized['input_ids']])\n",
        "    output_token_lengths = np.array([len(token) for token in output_batch_tokenized['input_ids']])\n",
        "    combined_token_lengths = input_token_lengths + output_token_lengths\n",
        "\n",
        "    # Filter the records that exceed token length specified\n",
        "    batch_filtered_indices = np.where((combined_token_lengths < max_total_length) & (input_token_lengths <= max_input_length))[0] # Using NumPy arrays for efficient filtering\n",
        "    adjusted_indices = [start_idx + idx for idx in batch_filtered_indices]\n",
        "    filtered_indices.extend(adjusted_indices) # concatenating the filtered indices to the main list outside the loop\n",
        "\n",
        "\n",
        "  return df.iloc[filtered_indices].reset_index(drop = True)\n",
        "\n",
        "df_validation = filter_records(df_validation, tokenizer)\n",
        "df_validation"
      ],
      "metadata": {
        "id": "ARcAuDrg7sEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Selecting Random Sample from validation Dataset for Inference**\n",
        "To remove any bias, the samples used for validation are selected at random from the filtered dataset"
      ],
      "metadata": {
        "id": "jGNtO7PdgHwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_validation_sample = df_validation.sample(n = 300, random_state = 42) # Selecting 300 random samples from the validation dataset\n",
        "\n",
        "\"\"\"Now we extract the sample dataset into lists\"\"\"\n",
        "focal_methods = df_validation_sample[\"input\"].tolist()\n",
        "validation_unit_tests = df_validation_sample[\"output\"].tolist()"
      ],
      "metadata": {
        "id": "IG1TNwDmgPE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Run Inference with the Finetuned Model**\n",
        "Inference is performed using the input focal methods from all 300 selected samples. The generated responses can then be compared with the actual test cases"
      ],
      "metadata": {
        "id": "TvqOi_xBgyA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Creating a function to generate test cases from the finetuned model through inference\"\"\"\n",
        "def generate_test_cases(focal_methods):\n",
        "  generated_test_cases = []\n",
        "\n",
        "  for focal_method in tqdm(focal_methods, desc = \"Generating Test Cases\", unit = \"case\"):\n",
        "    prompt = f\"---Focal Method---\\n{focal_method}\\n\\n---Unit Test---\\n\"  # Using the predefined chat template\n",
        "    pipe = pipeline(task = \"text-generation\", model = model, tokenizer = tokenizer, max_length = 1024)\n",
        "    result = pipe(prompt)\n",
        "    generated_test_cases.append(result[0]['generated_text'][len(prompt):]) # Appending each generated response to the main list outside the loop\n",
        "\n",
        "  return generated_test_cases\n",
        "\n",
        "generated_test_cases = generate_test_cases(focal_methods)\n",
        "\n",
        "df_validation_sample[\"generated_tests\"] = generated_test_cases # Adding a column for the generated responses in the validation dataframe"
      ],
      "metadata": {
        "id": "uFEXryBoO5Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create a Tokenized Version of the Validation Sample Dataset**\n",
        "Now, for calculating the evaluation metrics, the input, actual output, and generated output need to be tokenized."
      ],
      "metadata": {
        "id": "wKq037KxiLSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Tokenizing the inputs, true outputs, and generated outputs\"\"\"\n",
        "input_ids = tokenizer(df_validation_sample[\"input\"].tolist(), return_tensors = \"np\", padding = True)\n",
        "output_ids = tokenizer(df_validation_sample[\"output\"].tolist(), return_tensors = \"np\", padding = True)\n",
        "generated_test_ids = tokenizer(df_validation_sample[\"generated_tests\"].tolist(), return_tensors = \"np\", padding = True)\n",
        "df_validation_sample[\"input_token_ids\"] = input_ids[\"input_ids\"].tolist()\n",
        "df_validation_sample[\"output_token_ids\"] = output_ids[\"input_ids\"].tolist()\n",
        "df_validation_sample[\"generated_test_token_ids\"] = generated_test_ids[\"input_ids\"].tolist()\n",
        "\n",
        "df_final_validation = df_validation_sample.copy()\n",
        "df_final_validation"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fMrvapabiRm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluating the Accuracy of Generated Test Cases**\n",
        "Finally, the accuracy, precision, recall and f1 score for the generated test cases are calculated"
      ],
      "metadata": {
        "id": "a2iAN1p9htM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Creating a function to calculate all the evaluation metrics\"\"\"\n",
        "def calculate_metrics(df):\n",
        "  predicted_ids_complete = []\n",
        "  true_ids_complete = []\n",
        "\n",
        "  for _, row in df.iterrows(): # Iterating over the tokenized dataframe to retrieve tokens for actual outputs and generated outputs\n",
        "    predicted_ids = row[\"generated_test_token_ids\"]\n",
        "    true_ids = row[\"output_token_ids\"]\n",
        "\n",
        "    min_length = min(len(predicted_ids), len(true_ids)) # Aligning the lengths of the true and generated tokens to avoid any problems in calculation of the metrics\n",
        "    predicted_ids = predicted_ids[:min_length]\n",
        "    true_ids = true_ids[:min_length]\n",
        "\n",
        "    predicted_ids_complete.extend(predicted_ids)\n",
        "    true_ids_complete.extend(true_ids)\n",
        "\n",
        "  accuracy = accuracy_score(true_ids_complete, predicted_ids_complete)\n",
        "  precision = precision_score(true_ids_complete, predicted_ids_complete, average = \"macro\", zero_division = 1) # Setting zero division as 1 to avoid any zero division errors\n",
        "  recall = recall_score(true_ids_complete, predicted_ids_complete, average = \"macro\", zero_division = 1)\n",
        "  f1 = f1_score(true_ids_complete, predicted_ids_complete, average = \"macro\", zero_division = 1)\n",
        "\n",
        "  return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "metrics = calculate_metrics(df_final_validation)\n",
        "\n",
        "print(f\"Metrics: \\n{metrics}\") # Print the calculated Evaluation Metrics"
      ],
      "metadata": {
        "id": "Tc0CGgsJhyw_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}